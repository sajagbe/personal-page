<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>soajagbe</title>
    <link>http://localhost:1313/blog/</link>
    <description>Recent content on soajagbe</description>
    <generator>Hugo -- 0.134.2</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Mar 2025 12:20:32 -0500</lastBuildDate>
    <atom:link href="http://localhost:1313/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Scythe Trilogy</title>
      <link>http://localhost:1313/blog/scythe-trilogy/</link>
      <pubDate>Wed, 05 Mar 2025 12:20:32 -0500</pubDate>
      <guid>http://localhost:1313/blog/scythe-trilogy/</guid>
      <description>&lt;p&gt;I read the three books, the most I had read in a while.
In fact, I finished the last 2 in less than 2 weeeks from the time I picked up book 2 in February.&lt;/p&gt;
&lt;h3 id=&#34;on-the-main-characters&#34;&gt;On the main characters:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Faraday - Wise and human.&lt;/li&gt;
&lt;li&gt;Citra - Brilliant and strongwilled.&lt;/li&gt;
&lt;li&gt;Rowan - Ruthless and romantic.&lt;/li&gt;
&lt;li&gt;Curie - Protective and Respectable.&lt;/li&gt;
&lt;li&gt;Goddard - Cunny, Capricious but worse in the fictional world, Damningly Unrelatable.&lt;/li&gt;
&lt;li&gt;Tolliver - Liked this character. He was written like a person.&lt;/li&gt;
&lt;li&gt;Thunderhead - Provided a way to imagine God&amp;rsquo;s magnanimity.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;lines-that-made-me-feel-and-think&#34;&gt;Lines that made me feel and think.&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;â€œAnd Morrison was left to wrestle with himself. He hated doing that, because he was a formidable opponent.â€ This made me laugh, it was like looking in a mirror.&lt;/li&gt;
&lt;li&gt;â€œTo say he was an indecisive man was an understatement. He might have seemed confident to others, but the truth was heâ€™d never made a decision that he hadnâ€™t come to regret on some levelâ€”which is why he often let decisions be made for him.â€&lt;/li&gt;
&lt;li&gt;â€œI long for the luxury of being impractical. It would addâ€¦ textureâ€¦ to my existence.â€&lt;/li&gt;
&lt;li&gt;â€œâ€œTime passes so slowly, so smoothly,â€ it said. â€œAnd the atmospheric conditions! A tailwind at 8.6 kilometers per hour easing the flow of twenty-nine knots, the air at 70% humidity, but the numbers are nothing compared to the feel of it upon the skinâ€â€ Here, The Thunderhead highlights the satisfaction and even rarer, the beauty of experiencing things over analyzing them, something I stand to benefit from.&lt;/li&gt;
&lt;li&gt;â€œThe rest of the world saw them both as symbols. Intangible light to guide them in the darkness. She understood now why ancient peoples turned their heroes into constellations.â€ This solidified my understanding of the word â€œluminariesâ€ or naming stars and planets after heroes or great people. You want something to highlight and remind you of their greatness.  It gives hope when needed.&lt;/li&gt;
&lt;li&gt;â€œAs long as when bad things happened, they happened somewhere else, to someone they didnâ€™t know, it was not their problem.â€ This got at the heart of a problematic philosophy of mine. I have begun to fix it - especially in my mind - as I can better reason through the funnel from the current victims to me, but the practicality and allure of this thinking is not lost on me.&lt;/li&gt;
&lt;li&gt;â€œAs long as remorse is sincere, and one is willing to make recompense, there is no purpose to suffering.â€ Perhaps this is part of why God forgives, because our suffering post remorse and repentance has no point.&lt;/li&gt;
&lt;li&gt;â€œThou shalt lead an exemplary life in word and deed, and keep a journal of each and every day.â€ To me it seems that the latter is what evokes the first. Poring over myself - successes and mistakes - helps me understand or even define what a good life could look like for me, talk less of an exemplary one.&lt;/li&gt;
&lt;li&gt;â€œItâ€™s not about age, itâ€™s about stagnation.â€ A character said this when discussing choosing who to glean.&lt;/li&gt;
&lt;li&gt;â€œâ€œHeâ€™s always asking me how I am, and if thereâ€™s anything I need. And if there is, he always makes sure I get it. Just last week I asked for aâ€”â€ â€œâ€”No, not that kind of talking,â€ said Rowan, cutting her off. â€œI mean real talking. Has he ever hinted as to why you matter so much to him?â€â€ Here Rowan describes small and big talk. I enjoy the distinction.&lt;/li&gt;
&lt;li&gt;â€œWailing that the sky is falling does nothing to stop it.â€ Do something.&lt;/li&gt;
&lt;li&gt;â€œYou canâ€™t change the tide by spitting in the sea.â€ Accept what you canâ€™t change.&lt;/li&gt;
&lt;li&gt;â€œâ€œPeople are vessels,â€ Jeri had said to her. â€œThey hold whateverâ€™s poured into themâ€.â€&lt;/li&gt;
&lt;li&gt;â€œTime is never of the essence until someone decides that it is.â€&lt;/li&gt;
&lt;li&gt;â€œan infant is unaware of its own consciousness until it understands enough about the world to know that consciousness comes and goes, until it comes no more.â€ Perhaps the author here means consciousness as in life and death, or as in attention to the self.&lt;/li&gt;
&lt;li&gt;â€œbuilding a sandbox around a domineering child, then allowing that child to preside over it, frees the adults to do the real work.â€&lt;/li&gt;
&lt;li&gt;â€œtheater is the hallmark of ritual, and ritual is the touchstone of religionâ€. I noticed it in the regalia of Catholicism  and the performances of Pentecostalism. Some aspects of it are genuine, but it feels sometimes that the person on stage is performing.&lt;/li&gt;
&lt;li&gt;â€œIncrimination in a world without crime or nations.â€ This joke by Baba, made me rememeber that crime is primarily based on the nation.&lt;/li&gt;
&lt;li&gt;â€œItâ€™s not enough for you to knowâ€”youâ€™ve got find itâ€”so you can show others how to find it, too.â€ Itâ€™s one thing to know the answer, its a better thing to know how to figure out the answer. I enjoy experiencing the second.&lt;/li&gt;
&lt;li&gt;â€œNothing in history was a firsthand account, and things known really meant things that were allowed to be knownâ€&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;good-lines&#34;&gt;Good Lines&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&amp;ldquo;Goddard was brilliant at finding shapes in the clouds of his fury.&amp;rdquo; &amp;ldquo;More whim than wisdom. &amp;quot;&lt;/li&gt;
&lt;li&gt;â€œWe couldnâ€™t count our chickens before they were hatched.â€
â€œOr put our eggs in one basket,â€ added Baba. â€œI wonder which expression came first, the chickens or the eggs.â€ğŸ˜‚&lt;/li&gt;
&lt;li&gt;â€œDress quicklyâ€”we must leave in extreme haste,â€ he said.
â€œIâ€™ll be hasty in the morning,â€ she told him. ğŸ˜‚&lt;/li&gt;
&lt;li&gt;â€œWe are scythes, we are harmâ€™s way.â€&lt;/li&gt;
&lt;li&gt;â€œâ€¦preferred not to have an audience for their audiences.â€&lt;/li&gt;
&lt;li&gt;â€œA voice that resounded even when it whispered.â€ Cool possibility.&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Logistic Regression</title>
      <link>http://localhost:1313/blog/logistic-regression/</link>
      <pubDate>Wed, 12 Feb 2025 12:25:52 -0500</pubDate>
      <guid>http://localhost:1313/blog/logistic-regression/</guid>
      <description>&lt;h3 id=&#34;logistic-regression&#34;&gt;logistic regression&lt;/h3&gt;
&lt;p&gt;here the predicted outputs are binary, either a 0 or a 1, therefore the challenge is finding an equation that can translate the input features to either of the choices.&lt;/p&gt;
&lt;p&gt;To do this, they use the sigmoid function,viz:&lt;/p&gt;

  
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;âˆ’&lt;/mo&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
f(x)= \frac{1}{1+e^{-z}}
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
&lt;p&gt;where z is 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;âƒ—&lt;/mo&gt;&lt;/mover&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;.&lt;/mi&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;âƒ—&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\vec{w}.\vec{x} + b&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
.&lt;/p&gt;
&lt;p&gt;This means if z is high or simply +ve,  
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;âˆ’&lt;/mo&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;e^{-z}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 is very small and f(x) approaches 1 - thus, it is approximated as 1. And for the erverse, it is approximately 0. This way all the data is transformed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Z Score Normalization</title>
      <link>http://localhost:1313/blog/z-score-norm/</link>
      <pubDate>Wed, 05 Feb 2025 10:06:06 -0500</pubDate>
      <guid>http://localhost:1313/blog/z-score-norm/</guid>
      <description>&lt;h3 id=&#34;z-score-normalization&#34;&gt;z-score normalization:&lt;/h3&gt;
&lt;p&gt;This is a way to make all features in the dataset have the same unit (the 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;Ïƒ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\sigma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
).&lt;/p&gt;

  
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;Z&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;âˆ’&lt;/mo&gt;&lt;mi&gt;Î¼&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;Ïƒ&lt;/mi&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
Z = \frac{X - \mu}{\sigma}
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
&lt;p&gt;steps are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;get mean of feature from all examples (
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;Î¼&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\mu&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
)&lt;/li&gt;
&lt;li&gt;get deviation from mean for each item&lt;/li&gt;
&lt;li&gt;square values from (2) and sum them all&lt;/li&gt;
&lt;li&gt;Take average of (3), i.e. (3) / m where m is no. of examples&lt;/li&gt;
&lt;li&gt;Get 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\sqrt{(4)}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 (
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;Ïƒ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\sigma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When you do this for each sample feature in a feature set, the range will hence be from +b
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;Ïƒ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\sigma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 (gotten from the z norm for number highest from mean) to -b
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;Ïƒ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\sigma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 (gotten from the z norm for number lowest from mean) with 0 in the middle (i.e. the mean), where b is a scalar. so cool to understand why they are called &amp;ldquo;scalar&amp;rdquo;, because they literally scale what they multiply.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multifeature Linear Regression</title>
      <link>http://localhost:1313/blog/multifeature-linear-regression/</link>
      <pubDate>Sat, 25 Jan 2025 17:32:53 -0500</pubDate>
      <guid>http://localhost:1313/blog/multifeature-linear-regression/</guid>
      <description>&lt;h2 id=&#34;the-math&#34;&gt;&lt;strong&gt;the math:&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Logically same as univariate &lt;a href=&#34;https://soajagbe.xyz/blog/week2-2025/&#34;&gt;here&lt;/a&gt;,  just instead of single 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;w&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 and 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;x&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 there are multiple. It isalso more efficient to express all weights and features as vectors  
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;âƒ—&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\vec{w}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 and 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;âƒ—&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\vec{x}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
. This way their dot product (
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;âƒ—&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\vec{w}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mo&gt;â‹…&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\cdot&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;âƒ—&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\vec{x}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
)is efficiently handled.&lt;/p&gt;
&lt;p&gt;thus the equation becomes:&lt;/p&gt;

  
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;âƒ—&lt;/mo&gt;&lt;/mover&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;âƒ—&lt;/mo&gt;&lt;/mover&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;âƒ—&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;â‹…&lt;/mo&gt;&lt;msup&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;âƒ—&lt;/mo&gt;&lt;/mover&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
f_{\vec{w},b}(\vec{x}^{(i)}) = \vec w\cdot \vec{x}^{(i)} + b
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
&lt;p&gt;And the cost function 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;msubsup&gt;&lt;mo&gt;âˆ‘&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;âˆ’&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;âˆ’&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msup&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;J(w,b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
  would be changed to:&lt;/p&gt;

  
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;âƒ—&lt;/mo&gt;&lt;/mover&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;munderover&gt;&lt;mo&gt;âˆ‘&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;âˆ’&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/munderover&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;âƒ—&lt;/mo&gt;&lt;/mover&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;âƒ—&lt;/mo&gt;&lt;/mover&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;âˆ’&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msup&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
J(\vec{w},b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{\vec{w},b}(\vec{x}^{(i)}) - y^{(i)})^2
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
&lt;p&gt;For gradient descent and the derivatives:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Numpy and Tensors</title>
      <link>http://localhost:1313/blog/numpy-and-tensors/</link>
      <pubDate>Sat, 25 Jan 2025 17:30:00 -0500</pubDate>
      <guid>http://localhost:1313/blog/numpy-and-tensors/</guid>
      <description>&lt;p&gt;this is a summary of my understanding of the optional lab on covered numpy, arrays, vectors and matrices.&lt;/p&gt;
&lt;p&gt;because we will be working with huge sets of data and weâ€™ll be manipulating weights, properties and biases repeatedly, we need to do these operations efficiently. Thus, the &lt;code&gt;numpy&lt;/code&gt; package can help as it has vectors and matrix properties, which are faster and less memory intensive compared to hardcoded calculations.&lt;/p&gt;
&lt;p&gt;to create both matrices and vectors the &lt;code&gt;np.array&lt;/code&gt; syntax is typically used as such:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Richard Hamming, On Work</title>
      <link>http://localhost:1313/blog/hamming-you-and-your-research/</link>
      <pubDate>Thu, 23 Jan 2025 19:20:36 -0500</pubDate>
      <guid>http://localhost:1313/blog/hamming-you-and-your-research/</guid>
      <description>&lt;p&gt;Today, I read the transcript of Hamming&amp;rsquo;s You and your research talk found &lt;a href=&#34;https://www.cs.virginia.edu/~robins/YouAndYourResearch.html&#34;&gt;here&lt;/a&gt; and thatâ€™ll be the 2nd material of his Iâ€™ll come in contact with. I think itâ€™s a good idea to write down its essence for me.&lt;/p&gt;
&lt;h3 id=&#34;how-to-choose-work&#34;&gt;How to choose work:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Think before you work. Just hard work is not enough - it must be applied sensibly.&lt;/li&gt;
&lt;li&gt;Work on important problems. At least with these, the unavoidable struggle to make something could be worthwhile.&lt;/li&gt;
&lt;li&gt;Work on solutions you can see. â€œIt&amp;rsquo;s not the consequence that makes a problem important, it is that you have a reasonable attackâ€.&lt;/li&gt;
&lt;li&gt;Work on scalable solutions. Do not solve an isolated problem except as characteristic of a class.&lt;/li&gt;
&lt;li&gt;Work on things youâ€™re bold about. â€œOnce you get your courage up and believe that you can do important problems, then you canâ€.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;how-to-work&#34;&gt;How to work:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Donâ€™t let up. â€œThe good man gets on with the job, given what he&amp;rsquo;s got, and gets the best answer he canâ€. Alter the problem if necessary.&lt;/li&gt;
&lt;li&gt;Given two people of approximately the same ability and one person who works ten percent more than the other, the latter will more than twice outproduce the former.&lt;/li&gt;
&lt;li&gt;Practice tolerating ambiguity. Donâ€™t wait to know everything.&lt;/li&gt;
&lt;li&gt;When you choose one thing, let go of all else and keep your thoughts on it. Your subconscious will appreciate this clarity.&lt;/li&gt;
&lt;li&gt;You can educate your bosses.Â If you want to do something, don&amp;rsquo;t ask, do it. Present them with an accomplished fact. Don&amp;rsquo;t give them a chance to tell you â€œNoâ€œ.&lt;/li&gt;
&lt;li&gt;Learn to work with the system, and you will go as far as the system will support you. Or you can fight it steadily, as a small undeclared war, for the whole of your life.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;appearance of conforming&lt;/em&gt; gets you a long way.&lt;/li&gt;
&lt;li&gt;Learn to use yourself. Your ego is good when used for your benefit.&lt;/li&gt;
&lt;li&gt;For creative living, get your problems clear and refuse to look at any answers until you&amp;rsquo;ve thought of a solution. E.g read to be updated, not to copy answers.&lt;/li&gt;
&lt;li&gt;After acheiving competency in a field, pivot. It takes courage to say, â€œYes, I will give up my great reputationâ€.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;why-people-do-not-do-great-work&#34;&gt;Why people do not do great work:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;they don&amp;rsquo;t work on important problems,&lt;/li&gt;
&lt;li&gt;they don&amp;rsquo;t become emotionally involved,&lt;/li&gt;
&lt;li&gt;they don&amp;rsquo;t try and change what is difficult to what is easily done but is still important,&lt;/li&gt;
&lt;li&gt;they keep giving themselves alibis for this hesitation to change.&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    <item>
      <title>linear regression and gradient descent</title>
      <link>http://localhost:1313/blog/week2-2025/</link>
      <pubDate>Mon, 20 Jan 2025 16:19:21 -0500</pubDate>
      <guid>http://localhost:1313/blog/week2-2025/</guid>
      <description>&lt;p&gt;This is a short summary of the first week of the &lt;a href=&#34;https://www.coursera.org/learn/machine-learning&#34;&gt;machine learning course&lt;/a&gt; by Andrew Ng.&lt;/p&gt;
&lt;p&gt;First thing he covered was the difference between supervised and unsupervised learning, but I currently care about the former.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;supervised ml:&lt;/strong&gt;
Giving the computer a data set with sample answers of interest and telling it â€œfind the correlation between the dataset and the answers of interestâ€ or more simply, &amp;ldquo;learn how to get me the answers I care about given this dataset&amp;rdquo;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>ml start</title>
      <link>http://localhost:1313/blog/week1-2025/</link>
      <pubDate>Thu, 09 Jan 2025 19:40:33 -0500</pubDate>
      <guid>http://localhost:1313/blog/week1-2025/</guid>
      <description>&lt;p&gt;i want to continue posting online regularly.&lt;/p&gt;
&lt;p&gt;hence this first post.&lt;/p&gt;
&lt;p&gt;i started learning ml with andrew ng&amp;rsquo;s &lt;a href=&#34;https://www.coursera.org/specializations/machine-learning-introduction&#34;&gt;ml specialization&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;notes&#34;&gt;notes:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;last week, i took a hpc course from uc boulder and took some notes. i am going to make a post about it.&lt;/li&gt;
&lt;li&gt;i learned qchem prints S0 -&amp;gt; SX transition energies at SX optimized geometry, so this is technically not adiabatic excitation energy. at the back of my mind, i am going &amp;ldquo;why would they do that instead of SX energy at SX optimized geometry - S0 energy at S0 optimized geometry?&amp;rdquo;, which is indeed what i typically need when doing calculations. apparently, S2 energy at S2 energy - S0 energy at S2 minimum is called the vertical emission energy, while, the adiabatic excitation energy (AEE) is instead the S2 energy at the S2 minimum - S0 energy at the S0 geometry and i didn&amp;rsquo;t know the difference.&lt;/li&gt;
&lt;li&gt;i learned a trick to color the files and folders differently in the terminal - &lt;code&gt;ls --color&lt;/code&gt;. added this as an ls alias in my &lt;code&gt;.bashrc&lt;/code&gt; and i like the view.&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
  </channel>
</rss>
