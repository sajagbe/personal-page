<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>soajagbe</title>
    <link>http://localhost:1313/blog/</link>
    <description>Recent content on soajagbe</description>
    <generator>Hugo -- 0.134.2</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Mar 2025 12:20:32 -0500</lastBuildDate>
    <atom:link href="http://localhost:1313/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Scythe Trilogy</title>
      <link>http://localhost:1313/blog/scythe-trilogy/</link>
      <pubDate>Wed, 05 Mar 2025 12:20:32 -0500</pubDate>
      <guid>http://localhost:1313/blog/scythe-trilogy/</guid>
      <description>&lt;p&gt;I read the three books, the most I had read in a while.
In fact, I finished the last 2 in less than 2 weeeks from the time I picked up book 2 in February.&lt;/p&gt;
&lt;h3 id=&#34;on-the-main-characters&#34;&gt;On the main characters:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Faraday - Wise and human.&lt;/li&gt;
&lt;li&gt;Citra - Brilliant and strongwilled.&lt;/li&gt;
&lt;li&gt;Rowan - Ruthless and romantic.&lt;/li&gt;
&lt;li&gt;Curie - Protective and Respectable.&lt;/li&gt;
&lt;li&gt;Goddard - Cunny, Capricious but worse in the fictional world, Damningly Unrelatable.&lt;/li&gt;
&lt;li&gt;Tolliver - Liked this character. He was written like a person.&lt;/li&gt;
&lt;li&gt;Thunderhead - Provided a way to imagine God&amp;rsquo;s magnanimity.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;lines-that-made-me-feel-and-think&#34;&gt;Lines that made me feel and think.&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;“And Morrison was left to wrestle with himself. He hated doing that, because he was a formidable opponent.” This made me laugh, it was like looking in a mirror.&lt;/li&gt;
&lt;li&gt;“To say he was an indecisive man was an understatement. He might have seemed confident to others, but the truth was he’d never made a decision that he hadn’t come to regret on some level—which is why he often let decisions be made for him.”&lt;/li&gt;
&lt;li&gt;“I long for the luxury of being impractical. It would add… texture… to my existence.”&lt;/li&gt;
&lt;li&gt;““Time passes so slowly, so smoothly,” it said. “And the atmospheric conditions! A tailwind at 8.6 kilometers per hour easing the flow of twenty-nine knots, the air at 70% humidity, but the numbers are nothing compared to the feel of it upon the skin”” Here, The Thunderhead highlights the satisfaction and even rarer, the beauty of experiencing things over analyzing them, something I stand to benefit from.&lt;/li&gt;
&lt;li&gt;“The rest of the world saw them both as symbols. Intangible light to guide them in the darkness. She understood now why ancient peoples turned their heroes into constellations.” This solidified my understanding of the word “luminaries” or naming stars and planets after heroes or great people. You want something to highlight and remind you of their greatness.  It gives hope when needed.&lt;/li&gt;
&lt;li&gt;“As long as when bad things happened, they happened somewhere else, to someone they didn’t know, it was not their problem.” This got at the heart of a problematic philosophy of mine. I have begun to fix it - especially in my mind - as I can better reason through the funnel from the current victims to me, but the practicality and allure of this thinking is not lost on me.&lt;/li&gt;
&lt;li&gt;“As long as remorse is sincere, and one is willing to make recompense, there is no purpose to suffering.” Perhaps this is part of why God forgives, because our suffering post remorse and repentance has no point.&lt;/li&gt;
&lt;li&gt;“Thou shalt lead an exemplary life in word and deed, and keep a journal of each and every day.” To me it seems that the latter is what evokes the first. Poring over myself - successes and mistakes - helps me understand or even define what a good life could look like for me, talk less of an exemplary one.&lt;/li&gt;
&lt;li&gt;“It’s not about age, it’s about stagnation.” A character said this when discussing choosing who to glean.&lt;/li&gt;
&lt;li&gt;““He’s always asking me how I am, and if there’s anything I need. And if there is, he always makes sure I get it. Just last week I asked for a—” “—No, not that kind of talking,” said Rowan, cutting her off. “I mean real talking. Has he ever hinted as to why you matter so much to him?”” Here Rowan describes small and big talk. I enjoy the distinction.&lt;/li&gt;
&lt;li&gt;“Wailing that the sky is falling does nothing to stop it.” Do something.&lt;/li&gt;
&lt;li&gt;“You can’t change the tide by spitting in the sea.” Accept what you can’t change.&lt;/li&gt;
&lt;li&gt;““People are vessels,” Jeri had said to her. “They hold whatever’s poured into them”.”&lt;/li&gt;
&lt;li&gt;“Time is never of the essence until someone decides that it is.”&lt;/li&gt;
&lt;li&gt;“an infant is unaware of its own consciousness until it understands enough about the world to know that consciousness comes and goes, until it comes no more.” Perhaps the author here means consciousness as in life and death, or as in attention to the self.&lt;/li&gt;
&lt;li&gt;“building a sandbox around a domineering child, then allowing that child to preside over it, frees the adults to do the real work.”&lt;/li&gt;
&lt;li&gt;“theater is the hallmark of ritual, and ritual is the touchstone of religion”. I noticed it in the regalia of Catholicism  and the performances of Pentecostalism. Some aspects of it are genuine, but it feels sometimes that the person on stage is performing.&lt;/li&gt;
&lt;li&gt;“Incrimination in a world without crime or nations.” This joke by Baba, made me rememeber that crime is primarily based on the nation.&lt;/li&gt;
&lt;li&gt;“It’s not enough for you to know—you’ve got find it—so you can show others how to find it, too.” It’s one thing to know the answer, its a better thing to know how to figure out the answer. I enjoy experiencing the second.&lt;/li&gt;
&lt;li&gt;“Nothing in history was a firsthand account, and things known really meant things that were allowed to be known”&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;good-lines&#34;&gt;Good Lines&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&amp;ldquo;Goddard was brilliant at finding shapes in the clouds of his fury.&amp;rdquo; &amp;ldquo;More whim than wisdom. &amp;quot;&lt;/li&gt;
&lt;li&gt;“We couldn’t count our chickens before they were hatched.”
“Or put our eggs in one basket,” added Baba. “I wonder which expression came first, the chickens or the eggs.”😂&lt;/li&gt;
&lt;li&gt;“Dress quickly—we must leave in extreme haste,” he said.
“I’ll be hasty in the morning,” she told him. 😂&lt;/li&gt;
&lt;li&gt;“We are scythes, we are harm’s way.”&lt;/li&gt;
&lt;li&gt;“…preferred not to have an audience for their audiences.”&lt;/li&gt;
&lt;li&gt;“A voice that resounded even when it whispered.” Cool possibility.&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Logistic Regression</title>
      <link>http://localhost:1313/blog/logistic-regression/</link>
      <pubDate>Wed, 12 Feb 2025 12:25:52 -0500</pubDate>
      <guid>http://localhost:1313/blog/logistic-regression/</guid>
      <description>&lt;h3 id=&#34;logistic-regression&#34;&gt;logistic regression&lt;/h3&gt;
&lt;p&gt;here the predicted outputs are binary, either a 0 or a 1, therefore the challenge is finding an equation that can translate the input features to either of the choices.&lt;/p&gt;
&lt;p&gt;To do this, they use the sigmoid function,viz:&lt;/p&gt;

  
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
f(x)= \frac{1}{1+e^{-z}}
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
&lt;p&gt;where z is 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;.&lt;/mi&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\vec{w}.\vec{x} + b&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
.&lt;/p&gt;
&lt;p&gt;This means if z is high or simply +ve,  
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;e^{-z}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 is very small and f(x) approaches 1 - thus, it is approximated as 1. And for the erverse, it is approximately 0. This way all the data is transformed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Z Score Normalization</title>
      <link>http://localhost:1313/blog/z-score-norm/</link>
      <pubDate>Wed, 05 Feb 2025 10:06:06 -0500</pubDate>
      <guid>http://localhost:1313/blog/z-score-norm/</guid>
      <description>&lt;h3 id=&#34;z-score-normalization&#34;&gt;z-score normalization:&lt;/h3&gt;
&lt;p&gt;This is a way to make all features in the dataset have the same unit (the 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\sigma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
).&lt;/p&gt;

  
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;Z&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;μ&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
Z = \frac{X - \mu}{\sigma}
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
&lt;p&gt;steps are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;get mean of feature from all examples (
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;μ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\mu&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
)&lt;/li&gt;
&lt;li&gt;get deviation from mean for each item&lt;/li&gt;
&lt;li&gt;square values from (2) and sum them all&lt;/li&gt;
&lt;li&gt;Take average of (3), i.e. (3) / m where m is no. of examples&lt;/li&gt;
&lt;li&gt;Get 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\sqrt{(4)}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 (
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\sigma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When you do this for each sample feature in a feature set, the range will hence be from +b
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\sigma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 (gotten from the z norm for number highest from mean) to -b
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\sigma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 (gotten from the z norm for number lowest from mean) with 0 in the middle (i.e. the mean), where b is a scalar. so cool to understand why they are called &amp;ldquo;scalar&amp;rdquo;, because they literally scale what they multiply.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multifeature Linear Regression</title>
      <link>http://localhost:1313/blog/multifeature-linear-regression/</link>
      <pubDate>Sat, 25 Jan 2025 17:32:53 -0500</pubDate>
      <guid>http://localhost:1313/blog/multifeature-linear-regression/</guid>
      <description>&lt;h2 id=&#34;the-math&#34;&gt;&lt;strong&gt;the math:&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Logically same as univariate &lt;a href=&#34;https://soajagbe.xyz/blog/week2-2025/&#34;&gt;here&lt;/a&gt;,  just instead of single 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;w&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 and 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;x&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 there are multiple. It isalso more efficient to express all weights and features as vectors  
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\vec{w}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 and 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\vec{x}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
. This way their dot product (
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\vec{w}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\cdot&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\vec{x}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
)is efficiently handled.&lt;/p&gt;
&lt;p&gt;thus the equation becomes:&lt;/p&gt;

  
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;msup&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
f_{\vec{w},b}(\vec{x}^{(i)}) = \vec w\cdot \vec{x}^{(i)} + b
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
&lt;p&gt;And the cost function 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;msubsup&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msup&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;J(w,b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
  would be changed to:&lt;/p&gt;

  
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/munderover&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msup&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
J(\vec{w},b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{\vec{w},b}(\vec{x}^{(i)}) - y^{(i)})^2
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
&lt;p&gt;For gradient descent and the derivatives:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Numpy and Tensors</title>
      <link>http://localhost:1313/blog/numpy-and-tensors/</link>
      <pubDate>Sat, 25 Jan 2025 17:30:00 -0500</pubDate>
      <guid>http://localhost:1313/blog/numpy-and-tensors/</guid>
      <description>&lt;p&gt;this is a summary of my understanding of the optional lab on covered numpy, arrays, vectors and matrices.&lt;/p&gt;
&lt;p&gt;because we will be working with huge sets of data and we’ll be manipulating weights, properties and biases repeatedly, we need to do these operations efficiently. Thus, the &lt;code&gt;numpy&lt;/code&gt; package can help as it has vectors and matrix properties, which are faster and less memory intensive compared to hardcoded calculations.&lt;/p&gt;
&lt;p&gt;to create both matrices and vectors the &lt;code&gt;np.array&lt;/code&gt; syntax is typically used as such:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Richard Hamming, On Work</title>
      <link>http://localhost:1313/blog/hamming-you-and-your-research/</link>
      <pubDate>Thu, 23 Jan 2025 19:20:36 -0500</pubDate>
      <guid>http://localhost:1313/blog/hamming-you-and-your-research/</guid>
      <description>&lt;p&gt;Today, I read the transcript of Hamming&amp;rsquo;s You and your research talk found &lt;a href=&#34;https://www.cs.virginia.edu/~robins/YouAndYourResearch.html&#34;&gt;here&lt;/a&gt; and that’ll be the 2nd material of his I’ll come in contact with. I think it’s a good idea to write down its essence for me.&lt;/p&gt;
&lt;h3 id=&#34;how-to-choose-work&#34;&gt;How to choose work:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Think before you work. Just hard work is not enough - it must be applied sensibly.&lt;/li&gt;
&lt;li&gt;Work on important problems. At least with these, the unavoidable struggle to make something could be worthwhile.&lt;/li&gt;
&lt;li&gt;Work on solutions you can see. “It&amp;rsquo;s not the consequence that makes a problem important, it is that you have a reasonable attack”.&lt;/li&gt;
&lt;li&gt;Work on scalable solutions. Do not solve an isolated problem except as characteristic of a class.&lt;/li&gt;
&lt;li&gt;Work on things you’re bold about. “Once you get your courage up and believe that you can do important problems, then you can”.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;how-to-work&#34;&gt;How to work:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Don’t let up. “The good man gets on with the job, given what he&amp;rsquo;s got, and gets the best answer he can”. Alter the problem if necessary.&lt;/li&gt;
&lt;li&gt;Given two people of approximately the same ability and one person who works ten percent more than the other, the latter will more than twice outproduce the former.&lt;/li&gt;
&lt;li&gt;Practice tolerating ambiguity. Don’t wait to know everything.&lt;/li&gt;
&lt;li&gt;When you choose one thing, let go of all else and keep your thoughts on it. Your subconscious will appreciate this clarity.&lt;/li&gt;
&lt;li&gt;You can educate your bosses. If you want to do something, don&amp;rsquo;t ask, do it. Present them with an accomplished fact. Don&amp;rsquo;t give them a chance to tell you “No“.&lt;/li&gt;
&lt;li&gt;Learn to work with the system, and you will go as far as the system will support you. Or you can fight it steadily, as a small undeclared war, for the whole of your life.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;appearance of conforming&lt;/em&gt; gets you a long way.&lt;/li&gt;
&lt;li&gt;Learn to use yourself. Your ego is good when used for your benefit.&lt;/li&gt;
&lt;li&gt;For creative living, get your problems clear and refuse to look at any answers until you&amp;rsquo;ve thought of a solution. E.g read to be updated, not to copy answers.&lt;/li&gt;
&lt;li&gt;After acheiving competency in a field, pivot. It takes courage to say, “Yes, I will give up my great reputation”.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;why-people-do-not-do-great-work&#34;&gt;Why people do not do great work:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;they don&amp;rsquo;t work on important problems,&lt;/li&gt;
&lt;li&gt;they don&amp;rsquo;t become emotionally involved,&lt;/li&gt;
&lt;li&gt;they don&amp;rsquo;t try and change what is difficult to what is easily done but is still important,&lt;/li&gt;
&lt;li&gt;they keep giving themselves alibis for this hesitation to change.&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    <item>
      <title>linear regression and gradient descent</title>
      <link>http://localhost:1313/blog/week2-2025/</link>
      <pubDate>Mon, 20 Jan 2025 16:19:21 -0500</pubDate>
      <guid>http://localhost:1313/blog/week2-2025/</guid>
      <description>&lt;p&gt;This is a short summary of the first week of the &lt;a href=&#34;https://www.coursera.org/learn/machine-learning&#34;&gt;machine learning course&lt;/a&gt; by Andrew Ng.&lt;/p&gt;
&lt;p&gt;First thing he covered was the difference between supervised and unsupervised learning, but I currently care about the former.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;supervised ml:&lt;/strong&gt;
Giving the computer a data set with sample answers of interest and telling it “find the correlation between the dataset and the answers of interest” or more simply, &amp;ldquo;learn how to get me the answers I care about given this dataset&amp;rdquo;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>ml start</title>
      <link>http://localhost:1313/blog/week1-2025/</link>
      <pubDate>Thu, 09 Jan 2025 19:40:33 -0500</pubDate>
      <guid>http://localhost:1313/blog/week1-2025/</guid>
      <description>&lt;p&gt;i want to continue posting online regularly.&lt;/p&gt;
&lt;p&gt;hence this first post.&lt;/p&gt;
&lt;p&gt;i started learning ml with andrew ng&amp;rsquo;s &lt;a href=&#34;https://www.coursera.org/specializations/machine-learning-introduction&#34;&gt;ml specialization&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;notes&#34;&gt;notes:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;last week, i took a hpc course from uc boulder and took some notes. i am going to make a post about it.&lt;/li&gt;
&lt;li&gt;i learned qchem prints S0 -&amp;gt; SX transition energies at SX optimized geometry, so this is technically not adiabatic excitation energy. at the back of my mind, i am going &amp;ldquo;why would they do that instead of SX energy at SX optimized geometry - S0 energy at S0 optimized geometry?&amp;rdquo;, which is indeed what i typically need when doing calculations. apparently, S2 energy at S2 energy - S0 energy at S2 minimum is called the vertical emission energy, while, the adiabatic excitation energy (AEE) is instead the S2 energy at the S2 minimum - S0 energy at the S0 geometry and i didn&amp;rsquo;t know the difference.&lt;/li&gt;
&lt;li&gt;i learned a trick to color the files and folders differently in the terminal - &lt;code&gt;ls --color&lt;/code&gt;. added this as an ls alias in my &lt;code&gt;.bashrc&lt;/code&gt; and i like the view.&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
  </channel>
</rss>
