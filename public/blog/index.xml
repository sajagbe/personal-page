<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>soajagbe</title>
    <link>http://localhost:1313/blog/</link>
    <description>Recent content on soajagbe</description>
    <generator>Hugo -- 0.134.2</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 Feb 2025 12:25:52 -0500</lastBuildDate>
    <atom:link href="http://localhost:1313/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Logistic Regression</title>
      <link>http://localhost:1313/blog/logistic-regression/</link>
      <pubDate>Wed, 12 Feb 2025 12:25:52 -0500</pubDate>
      <guid>http://localhost:1313/blog/logistic-regression/</guid>
      <description>&lt;h3 id=&#34;logistic-regression&#34;&gt;logistic regression&lt;/h3&gt;
&lt;p&gt;here the predicted outputs are binary, either a 0 or a 1, therefore the challenge is finding an equation that can translate the input features to either of the choices.&lt;/p&gt;
&lt;p&gt;To do this, they use the sigmoid function,viz:&lt;/p&gt;

  
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
f(x)= \frac{1}{1+e^{-z}}
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
&lt;p&gt;where z is 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;mi mathvariant=&#34;normal&#34;&gt;.&lt;/mi&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\vec{w}.\vec{x} + b&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
.&lt;/p&gt;
&lt;p&gt;This means if z is high or simply +ve,  e^-z is very small and f(x) approaches 1 - and is approximated as 1. And for the erverse, it is approximately 0. This way all the data is transformed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Z Score Normalization</title>
      <link>http://localhost:1313/blog/z-score-norm/</link>
      <pubDate>Wed, 05 Feb 2025 10:06:06 -0500</pubDate>
      <guid>http://localhost:1313/blog/z-score-norm/</guid>
      <description>&lt;h3 id=&#34;z-score-normalization&#34;&gt;z-score normalization:&lt;/h3&gt;
&lt;p&gt;This is a way to make all features in the dataset have the same unit (the 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\sigma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
).&lt;/p&gt;

  
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;Z&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;μ&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
Z = \frac{X - \mu}{\sigma}
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
&lt;p&gt;steps are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;get mean of feature from all examples (
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;μ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\mu&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
)&lt;/li&gt;
&lt;li&gt;get deviation from mean for each item&lt;/li&gt;
&lt;li&gt;square values from (2) and sum them all&lt;/li&gt;
&lt;li&gt;Take average of (3), i.e. (3) / m where m is no. of examples&lt;/li&gt;
&lt;li&gt;Get 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\sqrt{(4)}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 (
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\sigma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When you do this for each sample feature in a feature set, the range will hence be from +b
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\sigma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 (gotten from the z norm for number highest from mean) to -b
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\sigma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 (gotten from the z norm for number lowest from mean) with 0 in the middle (i.e. the mean), where b is a scalar. so cool to understand why they are called &amp;ldquo;scalar&amp;rdquo;, because they literally scale what they multiply.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multifeature Linear Regression</title>
      <link>http://localhost:1313/blog/multifeature-linear-regression/</link>
      <pubDate>Sat, 25 Jan 2025 17:32:53 -0500</pubDate>
      <guid>http://localhost:1313/blog/multifeature-linear-regression/</guid>
      <description>&lt;h2 id=&#34;the-math&#34;&gt;&lt;strong&gt;the math:&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Logically same as univariate &lt;a href=&#34;https://soajagbe.xyz/blog/week2-2025/&#34;&gt;here&lt;/a&gt;,  just instead of single 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;w&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 and 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;x&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 there are multiple. It isalso more efficient to express all weights and features as vectors  
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\vec{w}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 and 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\vec{x}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
. This way their dot product (
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\vec{w}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\cdot&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\vec{x}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
)is efficiently handled.&lt;/p&gt;
&lt;p&gt;thus the equation becomes:&lt;/p&gt;

  
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;msup&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
f_{\vec{w},b}(\vec{x}^{(i)}) = \vec w\cdot \vec{x}^{(i)} + b
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
&lt;p&gt;And the cost function 
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;msubsup&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msup&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;J(w,b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
  would be changed to:&lt;/p&gt;

  
  &lt;span class=&#34;katex&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/munderover&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mrow&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;mo separator=&#34;true&#34;&gt;,&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mover accent=&#34;true&#34;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;⃗&lt;/mo&gt;&lt;/mover&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msup&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;
J(\vec{w},b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{\vec{w},b}(\vec{x}^{(i)}) - y^{(i)})^2
&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;
&lt;p&gt;For gradient descent and the derivatives:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Numpy and Tensors</title>
      <link>http://localhost:1313/blog/numpy-and-tensors/</link>
      <pubDate>Sat, 25 Jan 2025 17:30:00 -0500</pubDate>
      <guid>http://localhost:1313/blog/numpy-and-tensors/</guid>
      <description>&lt;p&gt;this is a summary of my understanding of the optional lab on covered numpy, arrays, vectors and matrices.&lt;/p&gt;
&lt;p&gt;because we will be working with huge sets of data and we’ll be manipulating weights, properties and biases repeatedly, we need to do these operations efficiently. Thus, the &lt;code&gt;numpy&lt;/code&gt; package can help as it has vectors and matrix properties, which are faster and less memory intensive compared to hardcoded calculations.&lt;/p&gt;
&lt;p&gt;to create both matrices and vectors the &lt;code&gt;np.array&lt;/code&gt; syntax is typically used as such:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Richard Hamming, On Work</title>
      <link>http://localhost:1313/blog/hamming-you-and-your-research/</link>
      <pubDate>Thu, 23 Jan 2025 19:20:36 -0500</pubDate>
      <guid>http://localhost:1313/blog/hamming-you-and-your-research/</guid>
      <description>&lt;p&gt;Today, I read the transcript of Hamming&amp;rsquo;s You and your research talk found &lt;a href=&#34;https://www.cs.virginia.edu/~robins/YouAndYourResearch.html&#34;&gt;here&lt;/a&gt; and that’ll be the 2nd material of his I’ll come in contact with. I think it’s a good idea to write down its essence for me.&lt;/p&gt;
&lt;h3 id=&#34;how-to-choose-work&#34;&gt;How to choose work:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Think before you work. Just hard work is not enough - it must be applied sensibly.&lt;/li&gt;
&lt;li&gt;Work on important problems. At least with these, the unavoidable struggle to make something could be worthwhile.&lt;/li&gt;
&lt;li&gt;Work on solutions you can see. “It&amp;rsquo;s not the consequence that makes a problem important, it is that you have a reasonable attack”.&lt;/li&gt;
&lt;li&gt;Work on scalable solutions. Do not solve an isolated problem except as characteristic of a class.&lt;/li&gt;
&lt;li&gt;Work on things you’re bold about. “Once you get your courage up and believe that you can do important problems, then you can”.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;how-to-work&#34;&gt;How to work:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Don’t let up. “The good man gets on with the job, given what he&amp;rsquo;s got, and gets the best answer he can”. Alter the problem if necessary.&lt;/li&gt;
&lt;li&gt;Given two people of approximately the same ability and one person who works ten percent more than the other, the latter will more than twice outproduce the former.&lt;/li&gt;
&lt;li&gt;Practice tolerating ambiguity. Don’t wait to know everything.&lt;/li&gt;
&lt;li&gt;When you choose one thing, let go of all else and keep your thoughts on it. Your subconscious will appreciate this clarity.&lt;/li&gt;
&lt;li&gt;You can educate your bosses. If you want to do something, don&amp;rsquo;t ask, do it. Present them with an accomplished fact. Don&amp;rsquo;t give them a chance to tell you “No“.&lt;/li&gt;
&lt;li&gt;Learn to work with the system, and you will go as far as the system will support you. Or you can fight it steadily, as a small undeclared war, for the whole of your life.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;appearance of conforming&lt;/em&gt; gets you a long way.&lt;/li&gt;
&lt;li&gt;Learn to use yourself. Your ego is good when used for your benefit.&lt;/li&gt;
&lt;li&gt;For creative living, get your problems clear and refuse to look at any answers until you&amp;rsquo;ve thought of a solution. E.g read to be updated, not to copy answers.&lt;/li&gt;
&lt;li&gt;After acheiving competency in a field, pivot. It takes courage to say, “Yes, I will give up my great reputation”.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;why-people-do-not-do-great-work&#34;&gt;Why people do not do great work:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;they don&amp;rsquo;t work on important problems,&lt;/li&gt;
&lt;li&gt;they don&amp;rsquo;t become emotionally involved,&lt;/li&gt;
&lt;li&gt;they don&amp;rsquo;t try and change what is difficult to what is easily done but is still important,&lt;/li&gt;
&lt;li&gt;they keep giving themselves alibis for this hesitation to change.&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    <item>
      <title>linear regression and gradient descent</title>
      <link>http://localhost:1313/blog/week2-2025/</link>
      <pubDate>Mon, 20 Jan 2025 16:19:21 -0500</pubDate>
      <guid>http://localhost:1313/blog/week2-2025/</guid>
      <description>&lt;p&gt;This is a short summary of the first week of the &lt;a href=&#34;https://www.coursera.org/learn/machine-learning&#34;&gt;machine learning course&lt;/a&gt; by Andrew Ng.&lt;/p&gt;
&lt;p&gt;First thing he covered was the difference between supervised and unsupervised learning, but I currently care about the former.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;supervised ml:&lt;/strong&gt;
Giving the computer a data set with sample answers of interest and telling it “find the correlation between the dataset and the answers of interest” or more simply, &amp;ldquo;learn how to get me the answers I care about given this dataset&amp;rdquo;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>ml start</title>
      <link>http://localhost:1313/blog/week1-2025/</link>
      <pubDate>Thu, 09 Jan 2025 19:40:33 -0500</pubDate>
      <guid>http://localhost:1313/blog/week1-2025/</guid>
      <description>&lt;p&gt;i want to continue posting online regularly.&lt;/p&gt;
&lt;p&gt;hence this first post.&lt;/p&gt;
&lt;p&gt;i started learning ml with andrew ng&amp;rsquo;s &lt;a href=&#34;https://www.coursera.org/specializations/machine-learning-introduction&#34;&gt;ml specialization&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;notes&#34;&gt;notes:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;last week, i took a hpc course from uc boulder and took some notes. i am going to make a post about it.&lt;/li&gt;
&lt;li&gt;i learned qchem prints S0 -&amp;gt; SX transition energies at SX optimized geometry, so this is technically not adiabatic excitation energy. at the back of my mind, i am going &amp;ldquo;why would they do that instead of SX energy at SX optimized geometry - S0 energy at S0 optimized geometry?&amp;rdquo;, which is indeed what i typically need when doing calculations. apparently, S2 energy at S2 energy - S0 energy at S2 minimum is called the vertical emission energy, while, the adiabatic excitation energy (AEE) is instead the S2 energy at the S2 minimum - S0 energy at the S0 geometry and i didn&amp;rsquo;t know the difference.&lt;/li&gt;
&lt;li&gt;i learned a trick to color the files and folders differently in the terminal - &lt;code&gt;ls --color&lt;/code&gt;. added this as an ls alias in my &lt;code&gt;.bashrc&lt;/code&gt; and i like the view.&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
  </channel>
</rss>
